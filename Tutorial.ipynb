{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources Used\n",
    "- wget.download('https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/_downloads/da4babe668a8afb093cc7776d7e630f3/generate_tfrecord.py')\n",
    "- Setup https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "SCRIPTS_PATH = 'Tensorflow/scripts'\n",
    "APIMODEL_PATH = 'Tensorflow/models'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\mohan\\AppData\\Local\\Temp\\ipykernel_16336\\66354810.py:3: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n"
     ]
    }
   ],
   "source": [
    "labels = [{'name':'Mask', 'id':1}, {'name':'NoMask', 'id':2}]\n",
    "\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mohan\\OneDrive\\Desktop\\real-sign-language\\Tensorflow\\scripts\\generate_tfrecord.py\", line 27, in <module>\n",
      "    import tensorflow.compat.v1 as tf\n",
      "  File \"C:\\Users\\mohan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'tensorflow.python'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mohan\\OneDrive\\Desktop\\real-sign-language\\Tensorflow\\scripts\\generate_tfrecord.py\", line 27, in <module>\n",
      "    import tensorflow.compat.v1 as tf\n",
      "  File \"C:\\Users\\mohan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\__init__.py\", line 40, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'tensorflow.python'\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download TF Models Pretrained Models from Tensorflow Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)\n",
      "error: 867 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n"
     ]
    }
   ],
   "source": [
    "!cd Tensorflow && git clone https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget.download('http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz')\n",
    "#!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRETRAINED_MODEL_PATH}\n",
    "#!cd {PRETRAINED_MODEL_PATH} && tar -zxvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: SyntaxWarning: invalid escape sequence '\\w'\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\n",
    "!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mobject_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_util\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mobject_detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline_pb2\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': ssd {\n",
      "  num_classes: 2\n",
      "  image_resizer {\n",
      "    fixed_shape_resizer {\n",
      "      height: 320\n",
      "      width: 320\n",
      "    }\n",
      "  }\n",
      "  feature_extractor {\n",
      "    type: \"ssd_mobilenet_v2_fpn_keras\"\n",
      "    depth_multiplier: 1.0\n",
      "    min_depth: 16\n",
      "    conv_hyperparams {\n",
      "      regularizer {\n",
      "        l2_regularizer {\n",
      "          weight: 3.9999998989515007e-05\n",
      "        }\n",
      "      }\n",
      "      initializer {\n",
      "        random_normal_initializer {\n",
      "          mean: 0.0\n",
      "          stddev: 0.009999999776482582\n",
      "        }\n",
      "      }\n",
      "      activation: RELU_6\n",
      "      batch_norm {\n",
      "        decay: 0.996999979019165\n",
      "        scale: true\n",
      "        epsilon: 0.0010000000474974513\n",
      "      }\n",
      "    }\n",
      "    use_depthwise: true\n",
      "    override_base_feature_extractor_hyperparams: true\n",
      "    fpn {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "      additional_layer_depth: 128\n",
      "    }\n",
      "  }\n",
      "  box_coder {\n",
      "    faster_rcnn_box_coder {\n",
      "      y_scale: 10.0\n",
      "      x_scale: 10.0\n",
      "      height_scale: 5.0\n",
      "      width_scale: 5.0\n",
      "    }\n",
      "  }\n",
      "  matcher {\n",
      "    argmax_matcher {\n",
      "      matched_threshold: 0.5\n",
      "      unmatched_threshold: 0.5\n",
      "      ignore_thresholds: false\n",
      "      negatives_lower_than_unmatched: true\n",
      "      force_match_for_each_row: true\n",
      "      use_matmul_gather: true\n",
      "    }\n",
      "  }\n",
      "  similarity_calculator {\n",
      "    iou_similarity {\n",
      "    }\n",
      "  }\n",
      "  box_predictor {\n",
      "    weight_shared_convolutional_box_predictor {\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.9999998989515007e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.009999999776482582\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.996999979019165\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000474974513\n",
      "        }\n",
      "      }\n",
      "      depth: 128\n",
      "      num_layers_before_predictor: 4\n",
      "      kernel_size: 3\n",
      "      class_prediction_bias_init: -4.599999904632568\n",
      "      share_prediction_tower: true\n",
      "      use_depthwise: true\n",
      "    }\n",
      "  }\n",
      "  anchor_generator {\n",
      "    multiscale_anchor_generator {\n",
      "      min_level: 3\n",
      "      max_level: 7\n",
      "      anchor_scale: 4.0\n",
      "      aspect_ratios: 1.0\n",
      "      aspect_ratios: 2.0\n",
      "      aspect_ratios: 0.5\n",
      "      scales_per_octave: 2\n",
      "    }\n",
      "  }\n",
      "  post_processing {\n",
      "    batch_non_max_suppression {\n",
      "      score_threshold: 9.99999993922529e-09\n",
      "      iou_threshold: 0.6000000238418579\n",
      "      max_detections_per_class: 100\n",
      "      max_total_detections: 100\n",
      "      use_static_shapes: false\n",
      "    }\n",
      "    score_converter: SIGMOID\n",
      "  }\n",
      "  normalize_loss_by_num_matches: true\n",
      "  loss {\n",
      "    localization_loss {\n",
      "      weighted_smooth_l1 {\n",
      "      }\n",
      "    }\n",
      "    classification_loss {\n",
      "      weighted_sigmoid_focal {\n",
      "        gamma: 2.0\n",
      "        alpha: 0.25\n",
      "      }\n",
      "    }\n",
      "    classification_weight: 1.0\n",
      "    localization_weight: 1.0\n",
      "  }\n",
      "  encode_background_as_zeros: true\n",
      "  normalize_loc_loss_by_codesize: true\n",
      "  inplace_batchnorm_update: true\n",
      "  freeze_batchnorm: false\n",
      "}\n",
      ", 'train_config': batch_size: 4\n",
      "data_augmentation_options {\n",
      "  random_horizontal_flip {\n",
      "  }\n",
      "}\n",
      "data_augmentation_options {\n",
      "  random_crop_image {\n",
      "    min_object_covered: 0.0\n",
      "    min_aspect_ratio: 0.75\n",
      "    max_aspect_ratio: 3.0\n",
      "    min_area: 0.75\n",
      "    max_area: 1.0\n",
      "    overlap_thresh: 0.0\n",
      "  }\n",
      "}\n",
      "sync_replicas: true\n",
      "optimizer {\n",
      "  momentum_optimizer {\n",
      "    learning_rate {\n",
      "      cosine_decay_learning_rate {\n",
      "        learning_rate_base: 0.07999999821186066\n",
      "        total_steps: 50000\n",
      "        warmup_learning_rate: 0.026666000485420227\n",
      "        warmup_steps: 1000\n",
      "      }\n",
      "    }\n",
      "    momentum_optimizer_value: 0.8999999761581421\n",
      "  }\n",
      "  use_moving_average: false\n",
      "}\n",
      "fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
      "num_steps: 50000\n",
      "startup_delay_steps: 0.0\n",
      "replicas_to_aggregate: 8\n",
      "max_number_of_boxes: 100\n",
      "unpad_groundtruth_tensors: false\n",
      "fine_tune_checkpoint_type: \"detection\"\n",
      ", 'train_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
      "tf_record_input_reader {\n",
      "  input_path: \"Tensorflow/workspace/annotations/train.record\"\n",
      "}\n",
      ", 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
      "use_moving_averages: false\n",
      ", 'eval_input_configs': [label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
      "shuffle: false\n",
      "num_epochs: 1\n",
      "tf_record_input_reader {\n",
      "  input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
      "}\n",
      "], 'eval_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
      "shuffle: false\n",
      "num_epochs: 1\n",
      "tf_record_input_reader {\n",
      "  input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
      "}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "from tensorflow.io import gfile\n",
    "\n",
    "CONFIG_PATH = \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\"\n",
    "def load_pipeline_config(config_path):\n",
    "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "\n",
    "    # Read and parse the pipeline config file\n",
    "    with gfile.GFile(config_path, \"r\") as f:\n",
    "        proto_str = f.read()\n",
    "        text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "    # Convert the parsed protobuf to a dictionary\n",
    "    configs = config_util.create_configs_from_pipeline_proto(pipeline_config)\n",
    "    return configs\n",
    "\n",
    "# Load the pipeline configuration\n",
    "configs = load_pipeline_config(CONFIG_PATH)\n",
    "\n",
    "# Print the loaded configurations for debugging\n",
    "print(configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = 2\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Load Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 04:39:05.104393: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_detections': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.], dtype=float32)>, 'detection_multiclass_scores': <tf.Tensor: shape=(1, 100, 91), dtype=float32, numpy=\n",
      "array([[[2.40962923e-04, 2.34315032e-03, 8.25872179e-04, ...,\n",
      "         5.31191996e-04, 1.88509771e-03, 4.45755257e-04],\n",
      "        [2.04591925e-04, 3.63720488e-03, 2.15955242e-03, ...,\n",
      "         9.21974133e-04, 2.74931057e-03, 1.09363045e-03],\n",
      "        [1.28045431e-04, 2.32595857e-03, 1.43585540e-03, ...,\n",
      "         1.03297585e-03, 1.93839485e-03, 6.19258790e-04],\n",
      "        ...,\n",
      "        [1.92099495e-03, 2.07633688e-03, 1.26298040e-03, ...,\n",
      "         8.85477813e-04, 3.06261843e-03, 1.27432519e-03],\n",
      "        [2.08881993e-05, 2.00390263e-04, 1.12296526e-04, ...,\n",
      "         2.25259078e-04, 8.79836734e-05, 3.06012262e-05],\n",
      "        [1.04016031e-03, 1.13887656e-02, 2.14778306e-03, ...,\n",
      "         3.09327291e-03, 4.77604615e-03, 6.80315634e-03]]], dtype=float32)>, 'detection_classes': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
      "array([[67., 59., 17., 51.,  7.,  9., 55., 56., 81., 67., 21., 82., 21.,\n",
      "        61.,  6., 64., 67., 51., 81., 82., 21., 21.,  1., 21.,  9., 64.,\n",
      "        59., 17., 59., 81., 55., 21., 79., 85.,  1.,  1., 53., 82., 67.,\n",
      "        67., 74., 51.,  8., 81.,  9., 21., 20., 74., 81., 55., 21.,  7.,\n",
      "        67., 81., 82.,  7., 61., 81.,  1., 21.,  7., 21., 67., 44., 74.,\n",
      "        21., 81., 82., 81., 61.,  1., 28., 20., 21., 21.,  6., 53., 82.,\n",
      "        67., 56., 81., 13., 82., 20.,  9., 82., 51., 56., 81., 74., 21.,\n",
      "        81.,  7., 82.,  7., 21., 51., 67., 20., 51.]], dtype=float32)>, 'detection_scores': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
      "array([[0.07362305, 0.05260816, 0.05181833, 0.04945543, 0.04884768,\n",
      "        0.04502587, 0.04428903, 0.04077207, 0.03895235, 0.03885148,\n",
      "        0.03850756, 0.037713  , 0.03740909, 0.03540837, 0.03507466,\n",
      "        0.03460605, 0.03368619, 0.03342806, 0.02974412, 0.02963996,\n",
      "        0.0285105 , 0.02839641, 0.02798391, 0.02729145, 0.02680125,\n",
      "        0.02675923, 0.02635963, 0.02612172, 0.02606519, 0.02596938,\n",
      "        0.02585121, 0.02545395, 0.02504659, 0.02503103, 0.02487679,\n",
      "        0.02463981, 0.02439123, 0.02412964, 0.02406286, 0.02368786,\n",
      "        0.02356596, 0.02342421, 0.02338435, 0.02316155, 0.02296645,\n",
      "        0.02268222, 0.02212598, 0.02194558, 0.02189229, 0.02173048,\n",
      "        0.02172865, 0.02164229, 0.02159489, 0.02127794, 0.02119855,\n",
      "        0.02101127, 0.02071541, 0.02043784, 0.01972315, 0.0196835 ,\n",
      "        0.01967276, 0.01961108, 0.01952496, 0.01945969, 0.01921874,\n",
      "        0.01901484, 0.01886417, 0.01876575, 0.01872067, 0.01869117,\n",
      "        0.01866907, 0.01843481, 0.01823035, 0.01812835, 0.01812204,\n",
      "        0.01802098, 0.01793653, 0.01787389, 0.01784854, 0.01780098,\n",
      "        0.01760813, 0.01758469, 0.01739554, 0.01738787, 0.01724944,\n",
      "        0.01724444, 0.01715985, 0.01713452, 0.0170956 , 0.01706251,\n",
      "        0.0168708 , 0.01680849, 0.01677655, 0.01673855, 0.01672709,\n",
      "        0.01671474, 0.01669589, 0.01664093, 0.01652673, 0.01643542]],\n",
      "      dtype=float32)>, 'detection_boxes': <tf.Tensor: shape=(1, 100, 4), dtype=float32, numpy=\n",
      "array([[[0.0000000e+00, 3.1307548e-02, 9.7827965e-01, 9.5794344e-01],\n",
      "        [7.0551276e-02, 5.3012818e-02, 9.1502428e-01, 9.3716955e-01],\n",
      "        [0.0000000e+00, 2.6900321e-02, 9.6936917e-01, 9.7823870e-01],\n",
      "        [7.0551276e-02, 5.3012818e-02, 9.1502428e-01, 9.3716955e-01],\n",
      "        [2.4828157e-01, 7.1497947e-02, 8.7865663e-01, 8.8353586e-01],\n",
      "        [2.0591721e-01, 6.5280944e-02, 9.1121304e-01, 8.7713921e-01],\n",
      "        [2.8388232e-02, 4.9644411e-02, 9.5419407e-01, 7.5348097e-01],\n",
      "        [1.6523533e-01, 7.2250640e-01, 2.5150165e-01, 9.0856624e-01],\n",
      "        [1.4208215e-01, 1.4725953e-02, 9.2724544e-01, 9.6513355e-01],\n",
      "        [8.4521234e-02, 2.9717058e-02, 9.7210318e-01, 6.1357856e-01],\n",
      "        [7.9050928e-01, 6.1963892e-01, 1.0000000e+00, 7.0929885e-01],\n",
      "        [0.0000000e+00, 2.6900321e-02, 9.6936917e-01, 9.7823870e-01],\n",
      "        [7.8365672e-01, 5.9042364e-01, 1.0000000e+00, 6.8607694e-01],\n",
      "        [2.8388232e-02, 4.9644411e-02, 9.5419407e-01, 7.5348097e-01],\n",
      "        [6.2080085e-02, 6.5323800e-02, 9.3632400e-01, 9.2967224e-01],\n",
      "        [0.0000000e+00, 1.0191968e-01, 9.7276545e-01, 9.7543585e-01],\n",
      "        [5.8148324e-02, 2.8320959e-01, 8.9181143e-01, 9.4062841e-01],\n",
      "        [8.4521234e-02, 2.9717058e-02, 9.7210318e-01, 6.1357856e-01],\n",
      "        [4.4518057e-01, 9.2457479e-01, 7.7088499e-01, 1.0000000e+00],\n",
      "        [4.5620024e-02, 4.2097747e-02, 8.9131135e-01, 6.5425503e-01],\n",
      "        [8.9189464e-01, 6.3641167e-01, 1.0000000e+00, 6.9803214e-01],\n",
      "        [7.4879426e-01, 5.5581605e-01, 1.0000000e+00, 6.6034234e-01],\n",
      "        [8.8757235e-01, 9.9080008e-01, 1.0000000e+00, 9.9758059e-01],\n",
      "        [7.6902854e-01, 3.0413914e-01, 1.0000000e+00, 4.0629089e-01],\n",
      "        [1.1993051e-02, 0.0000000e+00, 9.7700900e-01, 4.7370869e-01],\n",
      "        [4.5620024e-02, 4.2097747e-02, 8.9131135e-01, 6.5425503e-01],\n",
      "        [8.4521234e-02, 2.9717058e-02, 9.7210318e-01, 6.1357856e-01],\n",
      "        [4.5620024e-02, 4.2097747e-02, 8.9131135e-01, 6.5425503e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [0.0000000e+00, 0.0000000e+00, 4.2397439e-01, 3.1182006e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [9.0945894e-01, 6.1354840e-01, 1.0000000e+00, 6.6676652e-01],\n",
      "        [0.0000000e+00, 4.6515048e-02, 1.0000000e+00, 9.4480246e-01],\n",
      "        [7.0551276e-02, 5.3012818e-02, 9.1502428e-01, 9.3716955e-01],\n",
      "        [9.2856610e-01, 9.8977405e-01, 9.9855077e-01, 9.9737877e-01],\n",
      "        [9.5743990e-01, 9.6617395e-01, 9.8008144e-01, 9.7475141e-01],\n",
      "        [1.4208215e-01, 1.4725953e-02, 9.2724544e-01, 9.6513355e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [2.8376150e-01, 1.0959071e-01, 9.2677462e-01, 9.6939522e-01],\n",
      "        [6.4794463e-01, 5.0962538e-02, 1.0000000e+00, 1.0000000e+00],\n",
      "        [6.9702190e-01, 3.8652065e-01, 1.0000000e+00, 6.3005191e-01],\n",
      "        [1.2676398e-01, 1.6094953e-01, 2.5707215e-01, 3.5566038e-01],\n",
      "        [2.8824490e-01, 6.7280442e-02, 9.3515712e-01, 8.7150741e-01],\n",
      "        [5.8752716e-02, 7.0735976e-02, 4.1487196e-01, 3.7034857e-01],\n",
      "        [1.4113438e-01, 3.5172051e-01, 8.8232934e-01, 8.8975519e-01],\n",
      "        [8.5574728e-01, 6.0450363e-01, 1.0000000e+00, 6.7162335e-01],\n",
      "        [7.9050928e-01, 6.1963892e-01, 1.0000000e+00, 7.0929885e-01],\n",
      "        [7.2956890e-01, 3.9373428e-01, 1.0000000e+00, 5.4393679e-01],\n",
      "        [8.4521234e-02, 2.9717058e-02, 9.7210318e-01, 6.1357856e-01],\n",
      "        [8.4482849e-02, 0.0000000e+00, 8.8441378e-01, 4.8337346e-01],\n",
      "        [8.4869593e-01, 6.3021016e-01, 1.0000000e+00, 7.0088077e-01],\n",
      "        [1.0507035e-01, 7.6295137e-02, 9.6340227e-01, 6.7779791e-01],\n",
      "        [0.0000000e+00, 6.7098075e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [3.3210123e-01, 9.3078130e-01, 7.3126507e-01, 1.0000000e+00],\n",
      "        [5.8148324e-02, 2.8320959e-01, 8.9181143e-01, 9.4062841e-01],\n",
      "        [3.8925737e-01, 2.3772377e-01, 7.2328073e-01, 8.3942443e-01],\n",
      "        [5.8148324e-02, 2.8320959e-01, 8.9181143e-01, 9.4062841e-01],\n",
      "        [1.0281336e-01, 1.8545294e-01, 3.8896784e-01, 5.2019489e-01],\n",
      "        [8.4665346e-01, 9.9014437e-01, 1.0000000e+00, 9.9740028e-01],\n",
      "        [8.9718682e-01, 6.6069919e-01, 1.0000000e+00, 7.1495634e-01],\n",
      "        [1.9713730e-01, 3.2542452e-01, 9.6125954e-01, 9.0556765e-01],\n",
      "        [7.9806662e-01, 3.2508945e-01, 1.0000000e+00, 4.3373406e-01],\n",
      "        [4.5366007e-01, 8.5558981e-02, 1.0000000e+00, 8.9467883e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [7.2806120e-01, 4.4594008e-01, 1.0000000e+00, 5.9829146e-01],\n",
      "        [7.5815821e-01, 3.4979725e-01, 1.0000000e+00, 4.6447849e-01],\n",
      "        [0.0000000e+00, 1.8555620e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [3.8795954e-01, 9.2750305e-01, 7.4365646e-01, 1.0000000e+00],\n",
      "        [0.0000000e+00, 0.0000000e+00, 6.6133511e-01, 3.4561744e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [9.8481548e-01, 9.6528482e-01, 1.0000000e+00, 9.8825109e-01],\n",
      "        [0.0000000e+00, 2.6900321e-02, 9.6936917e-01, 9.7823870e-01],\n",
      "        [8.9718682e-01, 6.6069919e-01, 1.0000000e+00, 7.1495634e-01],\n",
      "        [7.9932255e-01, 2.8594151e-01, 1.0000000e+00, 3.8447955e-01],\n",
      "        [9.1523272e-01, 5.8960414e-01, 1.0000000e+00, 6.4118791e-01],\n",
      "        [3.5515523e-01, 3.2927126e-02, 9.0849495e-01, 8.5051548e-01],\n",
      "        [4.5620024e-02, 4.2097747e-02, 8.9131135e-01, 6.5425503e-01],\n",
      "        [7.1246207e-02, 4.3915361e-03, 1.0000000e+00, 9.4064392e-02],\n",
      "        [1.2950146e-01, 0.0000000e+00, 1.0000000e+00, 3.0488160e-01],\n",
      "        [1.5011209e-01, 7.0186698e-01, 2.6379824e-01, 8.7979341e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01],\n",
      "        [5.3855509e-02, 6.0070962e-02, 9.3231452e-01, 8.3813739e-01],\n",
      "        [1.9272196e-01, 8.8607419e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [7.3147088e-01, 5.6679076e-01, 1.0000000e+00, 6.8833655e-01],\n",
      "        [1.4258218e-01, 4.9480450e-01, 1.0000000e+00, 9.6887076e-01],\n",
      "        [0.0000000e+00, 6.7098075e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [0.0000000e+00, 1.8555620e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [1.9331464e-01, 7.2803777e-01, 2.6138532e-01, 9.4090515e-01],\n",
      "        [1.7164302e-01, 3.1020913e-01, 8.0973423e-01, 1.0000000e+00],\n",
      "        [7.3093879e-01, 3.6106583e-01, 1.0000000e+00, 4.9215910e-01],\n",
      "        [8.0319011e-01, 6.4517999e-01, 1.0000000e+00, 7.2389519e-01],\n",
      "        [0.0000000e+00, 6.7098075e-01, 1.0000000e+00, 1.0000000e+00],\n",
      "        [0.0000000e+00, 4.6515048e-02, 1.0000000e+00, 9.4480246e-01],\n",
      "        [2.2357218e-01, 9.3542081e-01, 7.2008693e-01, 1.0000000e+00],\n",
      "        [3.0690545e-01, 2.5951400e-01, 6.0778040e-01, 8.1030893e-01],\n",
      "        [7.7001971e-01, 2.6151675e-01, 1.0000000e+00, 3.5191637e-01],\n",
      "        [0.0000000e+00, 0.0000000e+00, 6.6133511e-01, 3.4561744e-01],\n",
      "        [3.4914982e-01, 0.0000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
      "        [8.0319011e-01, 6.4517999e-01, 1.0000000e+00, 7.2389519e-01],\n",
      "        [0.0000000e+00, 8.9220703e-04, 1.0000000e+00, 3.1444973e-01]]],\n",
      "      dtype=float32)>, 'raw_detection_boxes': <tf.Tensor: shape=(1, 12804, 4), dtype=float32, numpy=\n",
      "array([[[ 0.00907175,  0.0118995 ,  0.02405672,  0.03816519],\n",
      "        [-0.04300794, -0.04901042,  0.08154017,  0.09953573],\n",
      "        [ 0.0098893 , -0.00850921,  0.02288048,  0.04122598],\n",
      "        ...,\n",
      "        [ 0.07562506, -0.8228507 ,  1.5566578 ,  1.9890995 ],\n",
      "        [-0.09414595,  0.3338695 ,  1.480269  ,  1.3012037 ],\n",
      "        [-0.8363349 , -0.18970335,  2.2167578 ,  1.7399983 ]]],\n",
      "      dtype=float32)>, 'detection_anchor_indices': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
      "array([[12707., 12702., 12643., 12702., 12327., 12325., 12670.,  2114.,\n",
      "        12704., 12700.,  9041., 12643.,  9035., 12670., 12672., 12682.,\n",
      "        12652., 12700., 11158., 12640.,  9280.,  8789.,  9118.,  8729.,\n",
      "        12317., 12640., 12700., 12640., 12664., 12070., 12664.,  9274.,\n",
      "        12737., 12702.,  9358.,  9350., 12704., 12664., 12710., 12734.,\n",
      "        11701.,  1743., 12385., 12132., 12281.,  9034.,  9041., 11698.,\n",
      "        12700., 12660.,  9040., 12323., 12688., 10918., 12652., 12332.,\n",
      "        12652., 12140.,  8878.,  9286., 12341.,  8975., 12732., 12664.,\n",
      "        11704.,  8741., 12685., 11038., 12071., 12664.,  9590., 12643.,\n",
      "         9286.,  8963.,  9268., 12381., 12640., 12304., 12694.,  1863.,\n",
      "        12664., 12666., 12358., 11716., 12347., 12688., 12685.,  2360.,\n",
      "        12686., 11692.,  9047., 12688., 12737., 10678., 12272.,  8717.,\n",
      "        12071., 12735.,  9047., 12664.]], dtype=float32)>, 'raw_detection_scores': <tf.Tensor: shape=(1, 12804, 91), dtype=float32, numpy=\n",
      "array([[[0.00363437, 0.00274643, 0.00200334, ..., 0.00122433,\n",
      "         0.00152442, 0.00234495],\n",
      "        [0.00230811, 0.00213588, 0.00136394, ..., 0.0012705 ,\n",
      "         0.00107006, 0.00158459],\n",
      "        [0.00251749, 0.00518476, 0.00181949, ..., 0.00178648,\n",
      "         0.00152804, 0.00393779],\n",
      "        ...,\n",
      "        [0.00460613, 0.01047002, 0.0041258 , ..., 0.00400922,\n",
      "         0.00337534, 0.00322606],\n",
      "        [0.00381743, 0.00604227, 0.00327616, ..., 0.00294795,\n",
      "         0.00350851, 0.00269064],\n",
      "        [0.00394801, 0.00775033, 0.00457289, ..., 0.0038356 ,\n",
      "         0.00349342, 0.00317757]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Paths (Replace with your paths)\n",
    "SAVED_MODEL_PATH = \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model\"  # Path to the exported model directory\n",
    "CHECKPOINT_PATH = \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint\"   # Path to the checkpoint directory\n",
    "\n",
    "# Load the saved model\n",
    "detect_fn = tf.saved_model.load(SAVED_MODEL_PATH)\n",
    "\n",
    "# Define a function for running detection\n",
    "def run_inference(image):\n",
    "    \"\"\"\n",
    "    Perform object detection on an input image.\n",
    "    Args:\n",
    "        image: A NumPy array representing the input image.\n",
    "    Returns:\n",
    "        detections: A dictionary containing the detection results.\n",
    "    \"\"\"\n",
    "    # Convert the image to a tensor\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]  # Add batch dimension\n",
    "    \n",
    "    # Run inference\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with a sample image as NumPy array\n",
    "    sample_image = np.random.rand(640, 640, 3).astype(np.uint8)  # Placeholder image\n",
    "    \n",
    "    detections = run_inference(sample_image)\n",
    "    print(detections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Detect in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import string_int_label_map_pb2\n",
    "\n",
    "def load_label_map_custom(path):\n",
    "    \"\"\"Load label map from the given file path.\"\"\"\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    with open(path, 'r') as fid:  # Using built-in open\n",
    "        text_format.Merge(fid.read(), label_map)\n",
    "    return label_map\n",
    "\n",
    "def create_category_index_from_labelmap(label_map_path, use_display_name=True):\n",
    "    \"\"\"Create a category index from the label map.\"\"\"\n",
    "    label_map = load_label_map_custom(label_map_path)\n",
    "    categories = convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=use_display_name\n",
    "    )\n",
    "    return create_category_index(categories)\n",
    "\n",
    "def convert_label_map_to_categories(label_map, max_num_classes, use_display_name):\n",
    "    \"\"\"Convert label map to categories.\"\"\"\n",
    "    categories = []\n",
    "    for item in label_map.item:\n",
    "        if not 0 < item.id <= max_num_classes:\n",
    "            continue\n",
    "        categories.append({\n",
    "            'id': item.id,\n",
    "            'name': item.display_name if use_display_name and item.HasField('display_name') else item.name\n",
    "        })\n",
    "    return categories\n",
    "\n",
    "def create_category_index(categories):\n",
    "    \"\"\"Create category index from categories.\"\"\"\n",
    "    category_index = {}\n",
    "    for category in categories:\n",
    "        category_index[category['id']] = category\n",
    "    return category_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@9.994] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@9.995] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "# Setup capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Loading label map...\n",
      "Label map loaded.\n",
      "Starting video capture...\n",
      "Error: Unable to access video source.\n",
      "Warning: Unable to read frame.\n",
      "Video capture stopped and resources released.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@17.080] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@17.080] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.protos import string_int_label_map_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Constants\n",
    "SAVED_MODEL_PATH = 'Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model'  # Replace with your saved model path\n",
    "ANNOTATION_PATH = 'Tensorflow/workspace/annotations'  # Replace with your annotation folder path\n",
    "LABEL_MAP_FILE = f'{ANNOTATION_PATH}/label_map.pbtxt'\n",
    "VIDEO_SOURCE = 0  # Use 0 for webcam or a file path for video\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "detect_fn = tf.saved_model.load(SAVED_MODEL_PATH)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load the label map using `open`\n",
    "def load_label_map_custom(path):\n",
    "    \"\"\"Load label map from the given file path.\"\"\"\n",
    "    label_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    with open(path, 'r') as fid:  # Using built-in open instead of tf.io.gfile.GFile\n",
    "        text_format.Merge(fid.read(), label_map)\n",
    "    return label_map\n",
    "\n",
    "def create_category_index_from_labelmap(label_map_path, use_display_name=True):\n",
    "    \"\"\"Create a category index from the label map.\"\"\"\n",
    "    label_map = load_label_map_custom(label_map_path)\n",
    "    categories = []\n",
    "    for item in label_map.item:\n",
    "        if not 0 < item.id <= 90:  # Limit to 90 categories (COCO dataset standard)\n",
    "            continue\n",
    "        categories.append({\n",
    "            'id': item.id,\n",
    "            'name': item.display_name if use_display_name and item.HasField('display_name') else item.name\n",
    "        })\n",
    "    return {category['id']: category for category in categories}\n",
    "\n",
    "# Load the label map\n",
    "print(\"Loading label map...\")\n",
    "category_index = create_category_index_from_labelmap(LABEL_MAP_FILE, use_display_name=True)\n",
    "print(\"Label map loaded.\")\n",
    "\n",
    "# Function to process a frame and return the annotated frame\n",
    "def process_frame(frame):\n",
    "    # Convert to RGB (OpenCV uses BGR by default)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_np = np.array(frame_rgb, dtype=np.uint8)\n",
    "\n",
    "    # Prepare input tensor\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, axis=0), dtype=tf.uint8)\n",
    "\n",
    "    # Run inference\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    # Process detections\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Visualize detections\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        detections['detection_boxes'],\n",
    "        detections['detection_classes'] + 1,  # Offset for label IDs\n",
    "        detections['detection_scores'],\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        max_boxes_to_draw=5,\n",
    "        min_score_thresh=0.5,\n",
    "        agnostic_mode=False\n",
    "    )\n",
    "\n",
    "    # Convert back to BGR for OpenCV display\n",
    "    return cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Start video capture\n",
    "print(\"Starting video capture...\")\n",
    "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to access video source.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret or frame is None:\n",
    "            print(\"Warning: Unable to read frame.\")\n",
    "            break\n",
    "\n",
    "        # Process the frame\n",
    "        annotated_frame = process_frame(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Object Detection', cv2.resize(annotated_frame, (800, 600)))\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Video capture stopped and resources released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detect_fn(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
